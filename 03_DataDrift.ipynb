{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05ecc0b1-fbb9-4800-adad-8c7fa86638cb",
   "metadata": {},
   "source": [
    "# Menu <a class=\"anchor\" id=\"menu\"></a>\n",
    "\n",
    "* [1. Préparatifs pour les algorithmes non-supervisés](#setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ece261-9c7a-4580-885c-deebfe24a7cd",
   "metadata": {},
   "source": [
    "# Le data-drift\n",
    "\n",
    "**TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012c8fae-9494-4804-8227-fa19fc94df15",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# Préparatifs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "550a67b6-0cb6-495b-a328-2e3ffcf6bfc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# import re\n",
    "# import time\n",
    "# import math\n",
    "# import string\n",
    "import datetime\n",
    "# import unicodedata\n",
    "# import json\n",
    "from zipfile import ZipFile\n",
    "from functools import reduce\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import squarify\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "random_seed = 0\n",
    "np.random.seed(random_seed)\n",
    "cmap_ref = 'nipy_spectral'\n",
    "\n",
    "# import gc\n",
    "# gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63b7ee71-8a1f-4b0d-94a7-b6eba182a990",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_customer = pd.read_csv(os.path.join('data', \"olist_customers_dataset_clean.csv\"))\n",
    "data_orders = pd.read_csv(os.path.join('data', \"olist_orders_dataset_clean.csv\"))\n",
    "data_items = pd.read_csv(os.path.join('data', \"olist_order_items_dataset_clean.csv\"))\n",
    "data_payments = pd.read_csv(os.path.join('data', \"olist_order_payments_dataset_clean.csv\"))\n",
    "data_reviews = pd.read_csv(os.path.join('data', \"olist_order_reviews_dataset_clean.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2d7e15-2cff-4e6a-a81e-32a089a29525",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# 2. Feature Engineering <a class=\"anchor\" id=\"fe\"></a> [⇪](#menu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3752923d-3e71-43f9-97ec-648014133a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_date(x):\n",
    "    return datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "def prepare_data(since, until, verbose=0):\n",
    "\n",
    "    # Select the delivered orders using the provided purchase date range\n",
    "\n",
    "    if(verbose > 0):\n",
    "        print(f\"\\nSelection des commandes passées entre {since} et {until}\")\n",
    "\n",
    "    selection1 = data_orders[\n",
    "        (data_orders.order_purchase_timestamp >= since) & \n",
    "        (data_orders.order_purchase_timestamp < until) & \n",
    "        (data_orders.order_status == 'delivered')][['order_id', 'customer_id', 'order_purchase_timestamp']]\n",
    "\n",
    "    if(verbose > 1):\n",
    "        display(selection1.head(2))\n",
    "        display(selection1.shape)\n",
    "\n",
    "    # Add the `customer_unique_id` using the temporary `customer_id`\n",
    "    if(verbose > 0):\n",
    "        print(\"\\nAjout des ID unique des clients\")\n",
    "\n",
    "    selection2 = selection1.merge(data_customer[['customer_id', 'customer_unique_id']], on='customer_id', how='left')\n",
    "\n",
    "    if(verbose > 1):\n",
    "        display(selection2.head(2))\n",
    "        display(selection2.shape)\n",
    "\n",
    "    # Create a 'join' table that will be used for the various joints\n",
    "\n",
    "    if(verbose > 0):\n",
    "        print(\"\\nCréation d'une table de jointure\")\n",
    "\n",
    "    data_base_join = selection2[['order_id', 'customer_id', 'customer_unique_id']]\n",
    "\n",
    "    if (verbose > 1):\n",
    "        display(data_base_join.head(2))\n",
    "        display(data_base_join.shape)\n",
    "\n",
    "    # Feature 1\n",
    "    # Compute the `Recence` column for each order using the most recent provided date as reference\n",
    "\n",
    "    if(verbose > 0):\n",
    "        print(\"\\nCalcul de la recence pour chaque commande\")\n",
    "\n",
    "    selection2.order_purchase_timestamp = selection2.order_purchase_timestamp.apply(str_to_date)\n",
    "    selection2['recence'] = selection2.order_purchase_timestamp - str_to_date(until)\n",
    "    selection2['recence'] = selection2.recence.apply(lambda x: x.days)\n",
    "    selection2.drop(columns=['order_purchase_timestamp'], inplace=True)\n",
    "\n",
    "    if(verbose > 1):\n",
    "        display(selection2.head(2))\n",
    "        display(selection2.shape)\n",
    "\n",
    "    # Collect the most recent `Recence` value for each customer\n",
    "\n",
    "    if (verbose > 0):\n",
    "        print(\"\\nSelection de la plus petite valeur de recence pour chaque client\")\n",
    "\n",
    "    R_data = selection2[['customer_unique_id', 'recence']].groupby('customer_unique_id').min().reset_index()\n",
    "\n",
    "    if (verbose > 1):\n",
    "        display(R_data.head(2))\n",
    "        display(R_data.shape)\n",
    "\n",
    "    data_clustering = selection2[['customer_unique_id']].merge(R_data, on='customer_unique_id', how='left')\n",
    "\n",
    "    # Feature 2\n",
    "    # Compute the `Montant` column (total amount per order)\n",
    "\n",
    "    if (verbose > 0):\n",
    "        print(\"\\nCalcul du montant total pour chaque commande\")\n",
    "\n",
    "    total_amount_per_order = selection2[['order_id']].merge(\n",
    "        data_payments[['order_id', 'payment_value']], on='order_id', how='left'\n",
    "    ).groupby('order_id').sum().reset_index()\n",
    "    total_amount_per_order.rename(columns={'payment_value': 'montant'}, inplace=True)\n",
    "\n",
    "    if (verbose > 1):\n",
    "        display(total_amount_per_order.head(2))\n",
    "        display(total_amount_per_order.shape)\n",
    "\n",
    "    # Collect the sum of the `Montant` values for each customer\n",
    "\n",
    "    if (verbose > 0):\n",
    "        print(\"\\nCalcul de la somme des montants de commandes pour chaque client\")\n",
    "\n",
    "    M_data = selection2[['customer_unique_id', 'order_id']].merge(\n",
    "        total_amount_per_order, on='order_id', how='left'\n",
    "    ).groupby('customer_unique_id').sum().reset_index()\n",
    "\n",
    "    if (verbose > 1):\n",
    "        display(M_data.head(2))\n",
    "        display(M_data.shape)\n",
    "\n",
    "    data_clustering = data_clustering.merge(M_data, on='customer_unique_id', how='left')\n",
    "\n",
    "    # Feature 3\n",
    "    # Fetch 'review_score', 'answer_days' for each order\n",
    "\n",
    "    if (verbose > 0):\n",
    "        print(\"\\nRécupération des variables 'review_score', 'answer_days' pour chaque commande\")\n",
    "\n",
    "    data_fe = data_base_join.merge(\n",
    "        data_reviews[['order_id', 'review_score', 'answer_days']].groupby('order_id').last(),\n",
    "        on='order_id', how='left'\n",
    "    )\n",
    "\n",
    "    if (verbose > 1):\n",
    "        display(data_fe.head(2))\n",
    "        display(data_fe.shape)\n",
    "\n",
    "    # Collect the 'review_score' and 'answer_days' means for each customer\n",
    "\n",
    "    if (verbose > 0):\n",
    "        print(\"\\nCalcul des moyennes de 'review_score', 'answer_days' pour chaque client\")\n",
    "\n",
    "    data_fe2 = data_fe.groupby('customer_unique_id').mean().reset_index()\n",
    "\n",
    "    if (verbose > 1):\n",
    "        display(data_fe2.head(2))\n",
    "        display(data_fe2.shape)\n",
    "\n",
    "    data_clustering = data_clustering.merge(data_fe2, on='customer_unique_id', how='left')\n",
    "\n",
    "    # Feature 4 & 5\n",
    "    # Collect the max value for 'order_item_id' (which is the number of items in the order)\n",
    "\n",
    "    if (verbose > 0):\n",
    "        print(\"\\nRécupération de la valeur maximal de 'order_item_id' pour chaque commande\")\n",
    "\n",
    "    data_fe = data_items.groupby('order_id').last()[['order_item_id']].reset_index()\n",
    "    data_fe = data_base_join.merge(data_fe, on='order_id', how='left')\n",
    "\n",
    "    if (verbose > 1):\n",
    "        display(data_fe.head(2))\n",
    "        display(data_fe.shape)\n",
    "\n",
    "    # Collect the 'order_item_id' mean for each customer\n",
    "\n",
    "    if (verbose > 0):\n",
    "        print(\"\\nCalcul des moyennes de 'order_item_id' pour chaque client\")\n",
    "\n",
    "    data_fe2 = data_fe.groupby('customer_unique_id').mean().reset_index()\n",
    "    data_fe2.rename(columns={'order_item_id': 'mean_items'}, inplace=True)\n",
    "\n",
    "    if (verbose > 1):\n",
    "        display(data_fe2.head(2))\n",
    "        display(data_fe2.shape)\n",
    "\n",
    "    # Collect the 'order_item_id' max value for each customer\n",
    "\n",
    "    if (verbose > 0):\n",
    "        print(\"\\nCalcul de la valeur max de 'order_item_id' pour chaque client\")\n",
    "\n",
    "    data_fe3 = data_fe.groupby('customer_unique_id').sum().reset_index()\n",
    "    data_fe3.rename(columns={'order_item_id': 'total_items'}, inplace=True)\n",
    "\n",
    "    if (verbose > 1):\n",
    "        display(data_fe3.head(2))\n",
    "        display(data_fe3.shape)\n",
    "\n",
    "    data_clustering = data_clustering.merge(data_fe2, on='customer_unique_id', how='left')\n",
    "    data_clustering = data_clustering.merge(data_fe3, on='customer_unique_id', how='left')\n",
    "\n",
    "    # Merge\n",
    "    if (verbose > 0):\n",
    "        print(\"\\nFusion des différentes variables de chaque utilisateur unique en un seul jeu de données\")\n",
    "\n",
    "    if (verbose > 1):\n",
    "        display(data_clustering.head(2))\n",
    "        display(data_clustering.shape)\n",
    "\n",
    "    return data_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "20432659-8736-449e-8951-78e4b6e585c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_unique_id</th>\n",
       "      <th>recence</th>\n",
       "      <th>montant</th>\n",
       "      <th>review_score</th>\n",
       "      <th>answer_days</th>\n",
       "      <th>mean_items</th>\n",
       "      <th>total_items</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7c396fd4830fd04220f754e42b4e5bff</td>\n",
       "      <td>-362</td>\n",
       "      <td>82.82</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>af07308b275d755c9edb36a90c618231</td>\n",
       "      <td>-39</td>\n",
       "      <td>141.46</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3a653a41f6f9fc3d2a113cf8398680e8</td>\n",
       "      <td>-24</td>\n",
       "      <td>179.12</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7c142cf63193a1473d2e66489a9ae977</td>\n",
       "      <td>-287</td>\n",
       "      <td>72.20</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72632f0f9dd73dfee390c9b22eb56dd6</td>\n",
       "      <td>-200</td>\n",
       "      <td>28.62</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74208</th>\n",
       "      <td>a49e8e11e850592fe685ae3c64b40eca</td>\n",
       "      <td>-255</td>\n",
       "      <td>71.04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74209</th>\n",
       "      <td>c716cf2b5b86fb24257cffe9e7969df8</td>\n",
       "      <td>-332</td>\n",
       "      <td>106.79</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74210</th>\n",
       "      <td>da62f9e57a76d978d02ab5362c509660</td>\n",
       "      <td>-207</td>\n",
       "      <td>195.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74211</th>\n",
       "      <td>5097a5312c8b157bb7be58ae360ef43c</td>\n",
       "      <td>-236</td>\n",
       "      <td>441.16</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74212</th>\n",
       "      <td>60350aa974b26ff12caad89e55993bd6</td>\n",
       "      <td>-177</td>\n",
       "      <td>86.86</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74213 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     customer_unique_id  recence  montant  review_score  \\\n",
       "0      7c396fd4830fd04220f754e42b4e5bff     -362    82.82           4.5   \n",
       "1      af07308b275d755c9edb36a90c618231      -39   141.46           4.0   \n",
       "2      3a653a41f6f9fc3d2a113cf8398680e8      -24   179.12           5.0   \n",
       "3      7c142cf63193a1473d2e66489a9ae977     -287    72.20           5.0   \n",
       "4      72632f0f9dd73dfee390c9b22eb56dd6     -200    28.62           5.0   \n",
       "...                                 ...      ...      ...           ...   \n",
       "74208  a49e8e11e850592fe685ae3c64b40eca     -255    71.04           1.0   \n",
       "74209  c716cf2b5b86fb24257cffe9e7969df8     -332   106.79           5.0   \n",
       "74210  da62f9e57a76d978d02ab5362c509660     -207   195.00           4.0   \n",
       "74211  5097a5312c8b157bb7be58ae360ef43c     -236   441.16           2.0   \n",
       "74212  60350aa974b26ff12caad89e55993bd6     -177    86.86           5.0   \n",
       "\n",
       "       answer_days  mean_items  total_items  \n",
       "0              1.0         1.0            2  \n",
       "1              0.0         1.0            1  \n",
       "2              4.0         1.0            1  \n",
       "3              2.0         1.0            1  \n",
       "4              1.0         1.0            1  \n",
       "...            ...         ...          ...  \n",
       "74208          2.0         1.0            1  \n",
       "74209          2.0         2.0            2  \n",
       "74210          1.0         1.0            1  \n",
       "74211          1.0         2.0            2  \n",
       "74212          0.0         1.0            1  \n",
       "\n",
       "[74213 rows x 7 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B0 = prepare_data('2017-09-01 00:00:00', '2018-09-01 00:00:00', verbose=0)\n",
    "B0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d22d8cd-f916-4464-a8b9-a138f1ee7863",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# 3. Préparatifs pour les algorithmes d'apprentissage non-supervisés  <a class=\"anchor\" id=\"setup\"></a> [⇪](#menu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c7a81e-0955-4b4e-b07e-f7c206d30482",
   "metadata": {},
   "source": [
    "## 3.1 Suppression des ID <a class=\"anchor\" id=\"setup_1\"></a> [⇪](#menu)\n",
    "\n",
    "Supprimons la colonne `customer_unique_id` qui était utile pour faire les jointures, mais qui ne l'est pas pour le clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2110457d-f42a-4bbd-85f3-e0a52f6d9081",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cluster = data_clustering[[*data_clustering.select_dtypes(include='number')]].copy()\n",
    "X_cluster.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b951ac-6842-4192-ab87-10ff6d785635",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cluster.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1209cc10-a2ac-4f92-bebb-3cd46f03b793",
   "metadata": {},
   "source": [
    "## 3.2 Suppression des `NaN` <a class=\"anchor\" id=\"setup_2\"></a> [⇪](#menu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9c80cd-7270-4f96-95cf-838a2badd44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cluster.isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ac89a0-1bae-4986-b368-129bb20d5f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cluster.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c9b6c2-d297-4df6-9a99-60608645bda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cluster.isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fb0b0c-071e-484b-aa59-e2753afbf644",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cluster.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce384430-7ffe-45bb-8782-5904c26ec7e3",
   "metadata": {},
   "source": [
    "## 3.3 Traitement des `outliers` <a class=\"anchor\" id=\"setup_3\"></a> [⇪](#menu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad0884b-8aa0-4965-9805-c68597038931",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cluster.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c72253-12af-4d08-ac24-b596c1aaf3f1",
   "metadata": {},
   "source": [
    "#### Supprimons le dernier pourcentile des colonnes problèmatiques"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a2ab0bf4-be69-4717-952a-2121750d4afe",
   "metadata": {},
   "source": [
    "for col in ['montant', 'answer_days']:  # , 'mean_items', 'total_items']:\n",
    "    X_cluster = X_cluster[X_cluster[col] < X_cluster[col].quantile(0.9999)]\n",
    "\n",
    "X_cluster.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293180bd-bd59-471d-aaab-50e8680c5173",
   "metadata": {},
   "source": [
    "Après plusieurs aproches pour supprimer ou imputer les top outliers, je constate que quoi qu'il arrive **le clustering est moins bon sans ces outliers** sur presque tous les modèles que testés...<br>\n",
    "Je désactive donc cette cecllule pour garder ces top-outliers et les utiliser dans les recherches de clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2167f5-2772-483a-898e-c354d9e8274d",
   "metadata": {},
   "source": [
    "#### Supprimons les montant de 0"
   ]
  },
  {
   "cell_type": "raw",
   "id": "94e0f578-97cd-444e-93cb-c152c5546daa",
   "metadata": {},
   "source": [
    "X_cluster = X_cluster[ X_cluster.montant > 0 ]\n",
    "X_cluster.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8b29c563-1633-4424-a772-ca7607d8031a",
   "metadata": {},
   "source": [
    "X_cluster.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fdce67-cf8b-47cc-8bdf-8724b7b427a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 7))\n",
    "sns.scatterplot(x=X_cluster['recence'], y=X_cluster['montant'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a936e97-3735-42d2-8975-b1c162a348b5",
   "metadata": {},
   "source": [
    "## 3.3 `Normalisation` des variables <a class=\"anchor\" id=\"setup_4\"></a> [⇪](#menu)\n",
    "\n",
    "En effet, dans la mesure où nous allons utiliser des algorithmes basés sur la distance et ou l'on constate clairement des différences d'échelles, il est préférable de normaliser nos données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb26a0b8-fcfa-43f4-8dc1-600b4aea43cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler  # StandardScaler, LabelEncoder, OneHotEncoder, OrdinalEncoder,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c572b5c6-638a-4114-a5e3-b2bc1080e071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_normalizer(data):\n",
    "    \"\"\" Normalize the values of the provided dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: pandas' DataFrame\n",
    "        the dataset used as reference for the scaler\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    MinMaxScaler\n",
    "        the scaler instance\n",
    "    \"\"\"\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(data)\n",
    "\n",
    "    return scaler\n",
    "\n",
    "\n",
    "def normalize(data, scaler):\n",
    "    \"\"\" Transform the provided dataset using the provided scaler.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: pandas' DataFrame\n",
    "        the dataset that needs to be scaled\n",
    "    scaler: MinMaxScaler \n",
    "        the scaler instance to apply on the dataset\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        a new dataframe with the scaled values\n",
    "    \"\"\"\n",
    "\n",
    "    dt = data.copy()\n",
    "    dt = pd.DataFrame(scaler.transform(dt), index=dt.index, columns=dt.columns)\n",
    "\n",
    "    return dt\n",
    "\n",
    "\n",
    "def get_inverse_normalization(scaler, data):\n",
    "    \"\"\" Transform the provided dataset using the provided scaler back to the original scales\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: pandas' DataFrame\n",
    "        the dataset that needs to be scaled\n",
    "    scaler: MinMaxScaler \n",
    "        the scaler instance to apply on the dataset\n",
    "    columns: list\n",
    "        the list of columns to consider\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        a new dataframe with the scaled values\n",
    "    \"\"\"\n",
    "    \n",
    "    return pd.DataFrame(scaler.inverse_transform(data), columns=data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d3571f-9fe1-402f-99a9-e42e7118299e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scaler = fit_normalizer(X_cluster)\n",
    "X_cluster_norm = normalize(X_cluster, model_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8295299-0316-40d3-b80e-0be123d9fc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cluster_norm.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b7895c-fa2d-49b6-8e74-cfc75bea8d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cluster_norm.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf667a9-c836-493f-b87e-4638921717fa",
   "metadata": {},
   "source": [
    "## 3.4 Création des fonctions génériques <a class=\"anchor\" id=\"setup_5\"></a> [⇪](#menu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dd0f14-8d4d-4f62-ae5b-baf5d36b13e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Définissons la métrique utilisée par la fonction de recherche des hyper-paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d0bfbc-cb8c-4541-817c-9f962d6ca788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, silhouette_score, silhouette_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0211d1de-b604-483b-be48-7b8d1787ccf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_silhouette_scorer(estimator, X_ref):\n",
    "    estimator.fit(X_ref)\n",
    "    cluster_labels = estimator.labels_\n",
    "    num_labels = len(set(cluster_labels))\n",
    "    num_samples = len(X_ref.index)\n",
    "    if num_labels == 1 or num_labels == num_samples:\n",
    "        return -1\n",
    "    else:\n",
    "        return silhouette_score(X_ref, cluster_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dbd438-fb48-44b9-b65a-6672699fa18d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Définissons des fonctions pour afficher et enregistrer les scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157301ea-3ae0-440f-916f-0aa9a9ccf165",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from yellowbrick.cluster import SilhouetteVisualizer, InterclusterDistance\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5227b2-e3f6-449f-8fb5-4ef749aeb226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_silhouette(fitted_model, X_ref, silhouette_avg=None):\n",
    "\n",
    "    #fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    #fig.set_size_inches(15, 7)\n",
    "    fig = plt.figure(figsize=(15, 7))\n",
    "\n",
    "    if silhouette_avg is None:\n",
    "        silhouette_avg = cv_silhouette_scorer(fitted_model, X_ref)\n",
    "\n",
    "    cluster_labels = fitted_model.labels_\n",
    "    try:\n",
    "        sample_silhouette_values = silhouette_samples(X_ref, cluster_labels)\n",
    "    except Exception:\n",
    "        print(\"Il n'y a qu'un seul cluster, et silhouette_samples à besoin d'au moins 2 cluster...\")\n",
    "        return\n",
    "\n",
    "    if hasattr(fitted_model, 'n_clusters'):\n",
    "        n_clusters = fitted_model.n_clusters\n",
    "    elif hasattr(fitted_model, 'n_features_in_'):\n",
    "        # n_clusters = fitted_model.n_features_in_\n",
    "        n_clusters = pd.DataFrame(cluster_labels).nunique()[0]\n",
    "\n",
    "    # 1st Plot showing the silhouettes\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        #color = cm.cmap_ref(float(i) / n_clusters)\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "\n",
    "        ax1.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "        y_lower = y_upper + 10 \n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    if len(X_ref.columns) > 2:\n",
    "        ax2 = fig.add_subplot(122, projection='3d')\n",
    "        ax2.scatter(\n",
    "            X_training['recence'], X_training['montant'], X_training['review_score'],\n",
    "            marker=\"o\", lw=0, alpha=0.7,\n",
    "            c=colors, edgecolor=\"k\"\n",
    "        )\n",
    "        ax2.set_xlabel('recence')\n",
    "        ax2.set_ylabel('montant')\n",
    "        ax2.set_zlabel('review score')\n",
    "        ax2.set_title(\"The 3D visualization of the clustered data.\")\n",
    "    else:\n",
    "        ax2 = fig.add_subplot(122)\n",
    "        ax2.scatter(\n",
    "            #X_ref.iloc[:, 0], X_ref.iloc[:, 1],\n",
    "            X_training['recence'], X_training['montant'],\n",
    "            marker=\".\", lw=0, alpha=0.7, \n",
    "            c=cluster_labels, edgecolor=\"k\", cmap=cmap_ref,\n",
    "        )\n",
    "        ax2.set_xlabel('recence')\n",
    "        ax2.set_ylabel('montant')\n",
    "        ax2.set_title(\"The 2D visualization of the clustered data.\")\n",
    "\n",
    "    plt.suptitle(f\"Silhouette analysis with n_clusters = {n_clusters}\", fontsize=14, fontweight=\"bold\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ecdadc-31c9-437e-ac3a-2aa68d5f1168",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame(columns=['Method', 'params', 'best_params', 'silhouette', 'Training time', 'Inference time'])\n",
    "scores_path = 'data/scores.csv'\n",
    "scores_df.to_csv(scores_path, index=False)\n",
    "\n",
    "\n",
    "def get_clustering_scores(method_name, model, X_ref=None, param_grid=None, best_params=None, training_time=None, inference_time=None, register=False, **others):\n",
    "    \"\"\" Compute / Display / Save scores for the provided model\n",
    "\n",
    "    More precisely, it compute the scores then call various function to display and save them.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    method_name: str\n",
    "        the name used to identify the record in the list\n",
    "    model: \n",
    "        the model that needs to be evaluated\n",
    "    X_ref: list of lists\n",
    "        the X values used to get the predictions\n",
    "    param_grid: dict\n",
    "        the parameter grid used to get the provided scores\n",
    "    best_params: dict\n",
    "        the best parameters found with the gridsearch\n",
    "    training_time: float\n",
    "        the time needed for the fitting process\n",
    "    inference_time: float\n",
    "        the time needed for the prediction process\n",
    "    \"\"\"\n",
    "\n",
    "    if X_ref is None:\n",
    "        X_ref = X_cluster\n",
    "\n",
    "    silhouette_avg = cv_silhouette_scorer(model, X_ref)\n",
    "    scores = {'silhouette': silhouette_avg}\n",
    "\n",
    "    # Register score and replace if it already exists\n",
    "    if register:\n",
    "        save_score(method_name, param_grid, best_params, training_time, inference_time, **scores)\n",
    "\n",
    "    # Basic report\n",
    "    scores_str = \"\"\n",
    "    for key in scores.keys():\n",
    "        scores_str += f\"{key.upper().rjust(20)} : {scores[key]:.4f}\\n\"\n",
    "\n",
    "    print(f\"--- {method_name} ---\".ljust(100, '-'), \"\\n\\n\", scores_str, sep=\"\")\n",
    "\n",
    "    # Silhouette plot\n",
    "    # visualizer = SilhouetteVisualizer(model, colors='yellowbrick', is_fitted=True)\n",
    "    # visualizer.fit(X_ref)\n",
    "    # visualizer.show();\n",
    "    draw_silhouette(model, X_ref, silhouette_avg=silhouette_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2099ec-100d-44d9-9eef-4f22fabe5e17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_score(method_name, param_grid, best_params, training_time, inference_time, **scores):\n",
    "    \"\"\" Save the scores into the 'scores_df' DataFrame and to the 'scores_path' CSV file.\n",
    "    Each call to this function appends exactly one row to the DataFrame and hence to the CSV.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    method_name: str\n",
    "        the name used to identify the record in the list\n",
    "    param_grid: dict\n",
    "        the parameter grid used to get the provided scores\n",
    "    best_params: dict\n",
    "        the best parameters found with the gridsearch\n",
    "    training_time: float\n",
    "        the time needed for the fitting process\n",
    "    inference_time: float\n",
    "        the time needed for the prediction process\n",
    "    scores: list of parameters\n",
    "        the scores to register\n",
    "    \"\"\"\n",
    "\n",
    "    idx = np.where(scores_df.Method == method_name)[0]\n",
    "    idx = idx[0] if idx.size > 0 else len(scores_df.index)\n",
    "\n",
    "    silhouette = scores.get('silhouette', None)\n",
    "\n",
    "    scores_df.loc[idx] = [method_name, param_grid, best_params, silhouette, training_time, inference_time]\n",
    "    scores_df.to_csv(scores_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fa3b5b-497c-4acd-a6a8-4d753cea85d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_silhouette(model, data):\n",
    "\n",
    "    model = model.fit(data)\n",
    "    score = silhouette_score(data, model.labels_)\n",
    "    print(f\"Silhouette score moyen: {score:.3f}\")\n",
    "\n",
    "    # plt.figure(figsize=(10, 7))\n",
    "    # visualizer = SilhouetteVisualizer(model, colors='yellowbrick', is_fitted=True)\n",
    "    # visualizer.fit(data)\n",
    "    # visualizer.show();\n",
    "    draw_silhouette(model, data, silhouette_avg=score)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb3ba36-ec18-4a73-9542-f65fabd8ac59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_means(fitted_model, X_cluster):\n",
    "    X_labels = pd.DataFrame(fitted_model.labels_, columns=['label'])\n",
    "    X_results = X_cluster.merge(X_labels, left_index=True, right_index=True)\n",
    "\n",
    "    groups = {}\n",
    "    for i in range(fitted_model.n_clusters):\n",
    "        groups[i] = X_results[X_results.label == i].mean()\n",
    "\n",
    "    return pd.DataFrame.from_dict(groups).T.drop(columns=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b2d98f-5589-4a64-bd62-f75ddd7f1813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dendrogram(model, y_cut=0, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    fig = plt.figure(figsize=(15, 7))\n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "    plt.grid(False)\n",
    "    if type(y_cut) == list:\n",
    "        for y_c in y_cut:\n",
    "            plt.axhline(y=y_c, c='grey', lw=1, linestyle='dashed')\n",
    "    else:\n",
    "        plt.axhline(y=y_cut, c='grey', lw=1, linestyle='dashed')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_dendrogram_full(X_ref):\n",
    "    linked = linkage(X_ref, 'single')\n",
    "\n",
    "    # Plot dendrogram\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    dendrogram(linked,\n",
    "            orientation='top',\n",
    "            #labels=labelList,\n",
    "            distance_sort='descending',\n",
    "            show_leaf_counts=True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04225158-a14b-4954-91e6-87ccf77077cc",
   "metadata": {},
   "source": [
    "---\n",
    ">#### `TODO`  done ✅ | todo ❌ |\n",
    "> - ✅ Comme on a des algorithmes qui utilisent les distances, il est préférable de **normaliser les données** !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fdb939-f4ec-4db0-b647-78ab48e801c8",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### 🔔 Avant d'aller plus loin, réduisons le jeu de données pour que les algorithmes puissent travailler plus rapidement dans un premier temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5251c5-4263-421c-8450-f68071dbfa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_training = X_cluster_norm.sample(frac=0.25, random_state=random_seed).reset_index(drop=True).copy()\n",
    "#X_training = X_cluster_norm.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe32e76-d83b-440f-924c-b16d161ce3ab",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# 7. Modèle final <a class=\"anchor\" id=\"final\"></a> [⇪](#menu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fac99bd-e58f-4907-8945-d813517c399d",
   "metadata": {},
   "source": [
    "#### Reprenons le modèle le plus utile que nous avons pu découvrir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e43833-8aac-4300-bf5a-1434a7065082",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model = plot_silhouette(KMeans(n_clusters=9, random_state=random_seed), X_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8b353d-5bee-4733-a090-9bfcfb482def",
   "metadata": {},
   "source": [
    "#### Affichons les moyennes des variables par cluster *(dans les échelles d'origine)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce08ef7a-0e5d-4ba0-97ae-f58cf8b391a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_inverse_normalization(model_scaler, get_means(fitted_model, X_training)).sort_values(['recence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b7598c-a100-4e1d-9438-11fe67418689",
   "metadata": {},
   "source": [
    ">#### On constate que:\n",
    "> - les silhouettes montrent quelques points mal classés sur les groupes 4,5 et 8.\n",
    "> - on a un score honorable de 0.454.\n",
    "\n",
    "> - `cluster 7`: les clients *insatisfaits* qui ont *dépensé plus que la moyenne* et que l'on a *vu récemment*.\n",
    "> - `cluster 2`: les clients *insatisfaits* qui ont *dépensé plus que la moyenne* et que l'on a *vu il y a un certain temps*.\n",
    "> - `cluster 8`: les clients *insatisfaits* qui ont *dépensé plus que la moyenne* et que l'on a *vu il y a longtemps*.\n",
    "><br><br>\n",
    "> - `cluster 6`: les clients *modérément satisfaits* qui ont *dépensé moins que la moyenne* et que l'on a *vu récemment*.\n",
    "> - `cluster 4`: les clients *modérément satisfaits* et que l'on a *vu il y a un certain temps*.\n",
    "> - `cluster 5`: les clients *modérément satisfaits* et que l'on a *vu il y a longtemps*.\n",
    "><br><br>\n",
    "> - `cluster 1`: les clients *satisfaits* et que l'on a *vu récemment*.\n",
    "> - `cluster 3`: les clients *satisfaits* qui ont *dépensé moins que la moyenne* et que l'on a *vu il y a un certain temps*.\n",
    "> - `cluster 0`: les clients *satisfaits* et que l'on a *vu il y a longtemps*.\n",
    ">\n",
    "> Ce découpage est plutôt intéressant aussi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bf4140-0987-4441-8e7c-dbd8ecef0408",
   "metadata": {},
   "source": [
    "#### Affichons un parallel plot des centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b03ccd9-dbee-46b1-9329-ee6880d71cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 7))\n",
    "centroids = pd.DataFrame(fitted_model.cluster_centers_.T, index=X_training.columns).reset_index()\n",
    "pd.plotting.parallel_coordinates(centroids, class_column='index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3d0e4b-2042-48d9-8d50-0ec6c6167070",
   "metadata": {},
   "source": [
    ">#### On constate que sur ce modèle, ce sont principalement la `recence` et le `review_score` qui aident à déterminer les clusters...\n",
    ">Les autres variables n'ont qu'une influence à la marge."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvP5",
   "language": "python",
   "name": "venvp5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
